app = "vllm-demo"
primary_region = "ord"

[experimental]
entrypoint = "/start.sh"

[build]
image = "vllm/vllm-openai:v0.4.2"

[[mounts]]
source = "models"
destination = "/root/.cache/huggingface"

[http_service]
internal_port = 8000
force_https = true

[[vm]]
size = 'l40s'
gpus = 1

[[files]]
guest_path = "/start.sh"
local_path = "start.sh"

